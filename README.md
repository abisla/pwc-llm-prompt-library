# PwC LLM Prompt Library

This repository simulates how a Business Analyst might manage prompts, use cases, and structured outputs across departments using Large Language Models (LLMs) in a firm like PwC.

It includes:
- **Prompt version control** for different workflows (Advisory, Audit, Internal)
- **Structured metadata** in JSON for each prompt
- **Prompt evaluation logs** to compare output quality
- **Department-level folders** to reflect domain-specific GPTs (e.g. AdvisoryGPT, AuditGPT)

## ğŸ“ Structure
/prompts/
/advisory/ â†’ M&A intake prompts for client onboarding
/audit/ â†’ Audit risk triage prompts
/tests/ â†’ Output comparisons across prompt versions

## ğŸ’¡ How It Works

Each prompt is tracked by:
- `prompt_id`, `use_case`, and `version`
- Expected output format
- Last updated timestamp
- Model used (e.g., GPT-4)

## ğŸ” Sample Use Case

**AdvisoryGPT**
- Prompt: â€œProvide M&A deal detailsâ€
- Output: Bullet list of target, structure, and closing date
- Tracked version history with structured improvements

**AuditGPT**
- Prompt: â€œRate audit risk (High/Med/Low) based on client profileâ€
- Output: Risk score + justification

## ğŸ§  Why This Matters

This shows how LLMs can be operationalized like APIs â€” with version control, team-specific tuning, and performance tracking. It reflects how enterprise AI systems are built for trust, scale, and compliance.
